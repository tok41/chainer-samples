{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#VAE+GAN\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>VAE+GAN</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE+GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import six\n",
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "from StringIO import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.functions.loss.vae import gaussian_kl_divergence\n",
    "\n",
    "import chainer.optimizers as O\n",
    "import tqdm\n",
    "import time\n",
    "from IPython.display import display\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vaegan import Encoder, Decoder, Discriminator, EncDec\n",
    "from fauxtograph import get_paths, image_resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#paths = get_paths('../../sample_images/sample_train/')\n",
    "paths = get_paths('/home/tokita/projects/cinet/YouTubePriors_flv4/DividedImages/images_resize/sample_train_s/')\n",
    "print len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAEGAN(object):\n",
    "    def __init__(self, img_width=64, img_height=64, color_channels=3, encode_layers=[1000, 600, 300],\n",
    "                 decode_layers=[300, 800, 1000], disc_layers=[1000, 600, 300],\n",
    "                 kl_ratio=1.0, latent_width=500, flag_gpu=True, mode='convolution',\n",
    "                 enc_adam_alpha=0.0002, enc_adam_beta1=0.5, \n",
    "                 dec_adam_alpha=0.0002, dec_adam_beta1=0.5,\n",
    "                 disc_adam_alpha=0.0001, disc_adam_beta1=0.5,\n",
    "                 rectifier='clipped_relu', dropout_ratio=0.5):\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.color_channels = color_channels\n",
    "        self.encode_layers = encode_layers\n",
    "        self.decode_layers = decode_layers\n",
    "        self.disc_layers = disc_layers\n",
    "        self.kl_ratio = kl_ratio\n",
    "        self.latent_width = latent_width\n",
    "        self.flag_gpu = flag_gpu\n",
    "        self.mode = mode\n",
    "        self.enc_adam_alpha = enc_adam_alpha\n",
    "        self.enc_adam_beta1 = enc_adam_beta1\n",
    "        self.dec_adam_alpha = dec_adam_alpha\n",
    "        self.dec_adam_beta1 = dec_adam_beta1\n",
    "        self.disc_adam_alpha = disc_adam_alpha\n",
    "        self.disc_adam_beta1 = disc_adam_beta1\n",
    "        self.rectifier = rectifier\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        self.enc = Encoder(img_width=self.img_width, img_height=self.img_height, \n",
    "                           color_channels=self.color_channels, encode_layers=self.encode_layers,\n",
    "                           latent_width=self.latent_width, mode=self.mode)\n",
    "        self.dec = Decoder(img_width=self.img_width,img_height=self.img_height,\n",
    "                           color_channels=self.color_channels, decode_layers=self.decode_layers,\n",
    "                           latent_width=self.latent_width, mode=self.mode)\n",
    "        self.disc = Discriminator(img_width=self.img_width, img_height=self.img_height,\n",
    "                                  color_channels=self.color_channels, disc_layers=self.disc_layers,\n",
    "                                  latent_width=self.latent_width, mode=self.mode)\n",
    "        if self.flag_gpu:\n",
    "            self.enc = self.enc.to_gpu()\n",
    "            self.dec = self.dec.to_gpu()\n",
    "            self.disc = self.disc.to_gpu()\n",
    "\n",
    "        self.enc_opt = O.Adam(alpha=self.enc_adam_alpha, beta1=self.enc_adam_beta1)\n",
    "        self.dec_opt = O.Adam(alpha=self.dec_adam_alpha, beta1=self.dec_adam_beta1)\n",
    "        self.disc_opt = O.Adam(alpha=self.disc_adam_alpha, beta1=self.disc_adam_beta1)\n",
    "\n",
    "    def _encode(self, data, test=False):\n",
    "        x = self.enc(data, test=test)\n",
    "        mean, ln_var = F.split_axis(x, 2, 1)\n",
    "        samp = np.random.standard_normal(mean.data.shape).astype('float32')\n",
    "        samp = Variable(samp)\n",
    "        if self.flag_gpu:\n",
    "            samp.to_gpu()\n",
    "        z = samp * F.exp(0.5*ln_var) + mean\n",
    "\n",
    "        return z, mean, ln_var\n",
    "\n",
    "    def _decode(self, z, test=False):\n",
    "        x = self.dec(z, test=test, rectifier=self.rectifier)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _forward(self, batch, test=False):\n",
    "\n",
    "        # TrainingSetのEncodeとDecode\n",
    "        encoded, means, ln_vars = self._encode(batch, test=test)\n",
    "        rec = self._decode(encoded, test=test)\n",
    "        normer = reduce(lambda x, y: x*y, means.data.shape) # データ数\n",
    "        kl_loss = F.gaussian_kl_divergence(means, ln_vars)/normer\n",
    "        print 'means={}'.format(means.data.shape)\n",
    "        print 'ln_vars={}'.format(ln_vars.data.shape)\n",
    "        print 'kl_loss={}, normer={}'.format(kl_loss.data, normer)\n",
    "\n",
    "        # zのサンプル\n",
    "        samp_p = np.random.standard_normal(means.data.shape).astype('float32')\n",
    "        z_p = chainer.Variable(samp_p)\n",
    "\n",
    "        if self.flag_gpu:\n",
    "            z_p.to_gpu()\n",
    "\n",
    "        rec_p = self._decode(z_p)\n",
    "\n",
    "        disc_rec, conv_layer_rec = self.disc(rec, test=test, dropout_ratio=self.dropout_ratio)\n",
    "\n",
    "        disc_batch, conv_layer_batch = self.disc(batch, test=test, dropout_ratio=self.dropout_ratio)\n",
    "\n",
    "        disc_x_p, conv_layer_x_p = self.disc(rec_p, test=test, dropout_ratio=self.dropout_ratio)\n",
    "\n",
    "        dif_l = F.mean_squared_error(conv_layer_rec, conv_layer_batch)\n",
    "\n",
    "        return kl_loss, dif_l, disc_rec, disc_batch, disc_x_p\n",
    "\n",
    "    def transform(self, data, test=False):\n",
    "        #make sure that data has the right shape.\n",
    "        if not type(data) == Variable:\n",
    "            if len(data.shape) < 4:\n",
    "                data = data[np.newaxis]\n",
    "            if len(data.shape) != 4:\n",
    "                raise TypeError(\"Invalid dimensions for image data. Dim = %s.\\\n",
    "                     Must be 4d array.\" % str(data.shape))\n",
    "            if data.shape[1] != self.color_channels:\n",
    "                if data.shape[-1] == self.color_channels:\n",
    "                    data = data.transpose(0, 3, 1, 2)\n",
    "                else:\n",
    "                    raise TypeError(\"Invalid dimensions for image data. Dim = %s\"\n",
    "                                    % str(data.shape))\n",
    "            data = Variable(data)\n",
    "        else:\n",
    "            if len(data.data.shape) < 4:\n",
    "                data.data = data.data[np.newaxis]\n",
    "            if len(data.data.shape) != 4:\n",
    "                raise TypeError(\"Invalid dimensions for image data. Dim = %s.\\\n",
    "                     Must be 4d array.\" % str(data.data.shape))\n",
    "            if data.data.shape[1] != self.color_channels:\n",
    "                if data.data.shape[-1] == self.color_channels:\n",
    "                    data.data = data.data.transpose(0, 3, 1, 2)\n",
    "                else:\n",
    "                    raise TypeError(\"Invalid dimensions for image data. Dim = %s\"\n",
    "                                    % str(data.shape))\n",
    "\n",
    "        # Actual transformation.\n",
    "        if self.flag_gpu:\n",
    "            data.to_gpu()\n",
    "        z = self._encode(data, test=test)[0]\n",
    "\n",
    "        z.to_cpu()\n",
    "\n",
    "        return z.data\n",
    "\n",
    "    def inverse_transform(self, data, test=False):\n",
    "        if not type(data) == Variable:\n",
    "            if len(data.shape) < 2:\n",
    "                data = data[np.newaxis]\n",
    "            if len(data.shape) != 2:\n",
    "                raise TypeError(\"Invalid dimensions for latent data. Dim = %s.\\\n",
    "                     Must be a 2d array.\" % str(data.shape))\n",
    "            data = Variable(data)\n",
    "\n",
    "        else:\n",
    "            if len(data.data.shape) < 2:\n",
    "                data.data = data.data[np.newaxis]\n",
    "            if len(data.data.shape) != 2:\n",
    "                raise TypeError(\"Invalid dimensions for latent data. Dim = %s.\\\n",
    "                     Must be a 2d array.\" % str(data.data.shape))\n",
    "        assert data.data.shape[-1] == self.latent_width,\\\n",
    "            \"Latent shape %d != %d\" % (data.data.shape[-1], self.latent_width)\n",
    "\n",
    "        if self.flag_gpu:\n",
    "            data.to_gpu()\n",
    "        out = self._decode(data, test=test)\n",
    "\n",
    "        out.to_cpu()\n",
    "\n",
    "        if self.mode == 'linear':\n",
    "            final = out.data\n",
    "        else:\n",
    "            final = out.data.transpose(0, 2, 3, 1)\n",
    "\n",
    "        return final\n",
    "\n",
    "    def load_images(self, filepaths):\n",
    "        def read(fname):\n",
    "            im = Image.open(fname)\n",
    "            im = np.float32(im)\n",
    "            return im/255.\n",
    "        x_all = np.array([read(fname) for fname in tqdm.tqdm(filepaths)])\n",
    "        x_all = x_all.astype('float32')\n",
    "        if self.mode == 'convolution':\n",
    "            x_all = x_all.transpose(0, 3, 1, 2)\n",
    "        print(\"Image Files Loaded!\")\n",
    "        return x_all\n",
    "\n",
    "    def fit(self, img_data, gamma=1.0, save_freq=-1, pic_freq=-1, n_epochs=100, batch_size=100,\n",
    "            weight_decay=True,  model_path='./VAEGAN_training_model/', img_path='./VAEGAN_training_images/',\n",
    "            img_out_width=10, mirroring=False):\n",
    "        width = img_out_width\n",
    "        self.enc_opt.setup(self.enc)\n",
    "        self.dec_opt.setup(self.dec)\n",
    "        self.disc_opt.setup(self.disc)\n",
    "\n",
    "        if weight_decay:\n",
    "            self.enc_opt.add_hook(chainer.optimizer.WeightDecay(0.00001))\n",
    "            self.dec_opt.add_hook(chainer.optimizer.WeightDecay(0.00001))\n",
    "            self.disc_opt.add_hook(chainer.optimizer.WeightDecay(0.00001))\n",
    "\n",
    "        n_data = img_data.shape[0]\n",
    "\n",
    "        batch_iter = list(range(0, n_data, batch_size))\n",
    "        n_batches = len(batch_iter)\n",
    "\n",
    "        c_samples = np.random.standard_normal((width, self.latent_width)).astype(np.float32)\n",
    "        save_counter = 0\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            print('epoch: %i' % epoch)\n",
    "            t1 = time.time()\n",
    "            indexes = np.random.permutation(n_data)\n",
    "            sum_l_enc = 0.\n",
    "            sum_l_dec = 0.\n",
    "            sum_l_disc = 0.\n",
    "\n",
    "            sum_l_gan = 0.\n",
    "            sum_l_like = 0.\n",
    "            sum_l_prior = 0.\n",
    "            count = 0\n",
    "            for i in tqdm.tqdm(batch_iter):\n",
    "                x = img_data[indexes[i: i + batch_size]]\n",
    "                size = x.shape[0]\n",
    "                if mirroring:\n",
    "                    for j in range(size):\n",
    "                        if np.random.randint(2):\n",
    "                            x[j, :, :, :] = x[j, :, :, ::-1]\n",
    "                x_batch = Variable(x)\n",
    "                zeros = Variable(np.zeros(size, dtype=np.int32))\n",
    "                ones = Variable(np.ones(size, dtype=np.int32))\n",
    "\n",
    "                if self.flag_gpu:\n",
    "                    x_batch.to_gpu()\n",
    "                    zeros.to_gpu()\n",
    "                    ones.to_gpu()\n",
    "\n",
    "                # kl_loss : VAE中間表現のKL正則化ロス\n",
    "                # dif_l : Discriminatorの中間層出力のMSE(学習データセットと再構成画像の中間出力のMSE)\n",
    "                # disc_{rec, batch, samp} : Discriminator出力(2次元)\n",
    "                kl_loss, dif_l, disc_rec, disc_batch, disc_samp = self._forward(x_batch)\n",
    "\n",
    "                # Discriminator出力のloss計算\n",
    "                L_batch_GAN = F.softmax_cross_entropy(disc_batch, ones)\n",
    "                L_rec_GAN = F.softmax_cross_entropy(disc_rec, zeros)\n",
    "                L_samp_GAN = F.softmax_cross_entropy(disc_samp, zeros)\n",
    "\n",
    "                l_gan = (L_batch_GAN + L_rec_GAN + L_samp_GAN)/3.\n",
    "                l_like = dif_l\n",
    "                l_prior = kl_loss\n",
    "\n",
    "                enc_loss = self.kl_ratio*l_prior + l_like\n",
    "                dec_loss = gamma*l_like - l_gan\n",
    "                disc_loss = l_gan\n",
    "\n",
    "                self.enc_opt.zero_grads()\n",
    "                enc_loss.backward()\n",
    "                self.enc_opt.update()\n",
    "\n",
    "                self.dec_opt.zero_grads()\n",
    "                dec_loss.backward()\n",
    "                self.dec_opt.update()\n",
    "\n",
    "                self.disc_opt.zero_grads()\n",
    "                disc_loss.backward()\n",
    "                self.disc_opt.update()\n",
    "\n",
    "                sum_l_enc += enc_loss.data\n",
    "                sum_l_dec += dec_loss.data\n",
    "                sum_l_disc += disc_loss.data\n",
    "\n",
    "                sum_l_gan += l_gan.data\n",
    "                sum_l_like += l_like.data\n",
    "                sum_l_prior += l_prior.data\n",
    "                count += 1\n",
    "\n",
    "                #plot_data = img_data[indexes[:width]]\n",
    "\n",
    "            sum_l_enc /= n_batches\n",
    "            sum_l_dec /= n_batches\n",
    "            sum_l_disc /= n_batches\n",
    "            sum_l_gan /= n_batches\n",
    "            sum_l_like /= n_batches\n",
    "            sum_l_prior /= n_batches\n",
    "            msg = \"enc_loss = {0}, dec_loss = {1} , disc_loss = {2}\"\n",
    "            msg2 = \"gan_loss = {0}, sim_loss = {1}, kl_loss = {2}\"\n",
    "            print(msg.format(sum_l_enc, sum_l_dec, sum_l_disc))\n",
    "            print(msg2.format(sum_l_gan, sum_l_like, sum_l_prior))\n",
    "            t_diff = time.time()-t1\n",
    "            print(\"time: %f\\n\\n\" % t_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vg = VAEGAN(img_width=96, img_height=96, flag_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 126.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Files Loaded!\n",
      "image_data_shape = (200, 3, 96, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 画像ファイルのロード、正規化、transpose\n",
    "x_all = vg.load_images(paths)\n",
    "print 'image_data_shape = {}'.format(x_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "means=(100, 500)\n",
      "ln_vars=(100, 500)\n",
      "kl_loss=0.549522101879, normer=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means=(100, 500)\n",
      "ln_vars=(100, 500)\n",
      "kl_loss=0.500142037868, normer=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_loss = 2.35377192497, dec_loss = 0.963260769844 , disc_loss = 0.865679144859\n",
      "gan_loss = 0.865679144859, sim_loss = 1.8289399147, kl_loss = 0.524832069874\n",
      "time: 4.365414\n",
      "\n",
      "\n",
      "epoch: 2\n",
      "means=(100, 500)\n",
      "ln_vars=(100, 500)\n",
      "kl_loss=0.489462792873, normer=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means=(100, 500)\n",
      "ln_vars=(100, 500)\n",
      "kl_loss=0.495981395245, normer=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_loss = 2.23263502121, dec_loss = 0.977942347527 , disc_loss = 0.761970579624\n",
      "gan_loss = 0.761970579624, sim_loss = 1.73991298676, kl_loss = 0.492722094059\n",
      "time: 1.138272\n",
      "\n",
      "\n",
      "epoch: 3\n",
      "means=(100, 500)\n",
      "ln_vars=(100, 500)\n",
      "kl_loss=0.469831615686, normer=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means=(100, 500)\n",
      "ln_vars=(100, 500)\n",
      "kl_loss=0.452972173691, normer=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_loss = 2.15627789497, dec_loss = 0.99509626627 , disc_loss = 0.699779748917\n",
      "gan_loss = 0.699779748917, sim_loss = 1.69487595558, kl_loss = 0.461401879787\n",
      "time: 1.135749\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vg.fit(x_all, n_epochs=3, mirroring=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = 10, vg.latent_width\n",
    "random_data = np.random.standard_normal(shape).astype('f')*3.\n",
    "images = vg.inverse_transform(random_data, test=True)\n",
    "plt.figure(figsize=(16,3))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
