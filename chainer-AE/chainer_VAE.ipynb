{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChainerでVAEを書いてみる\n",
    "* 以下のページを参考に\n",
    "  * https://github.com/RyotaKatoh/chainer-Variational-AutoEncoder\n",
    "* VAEについては以下を参照した\n",
    "  * http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf\n",
    "  * http://deeplearning.jp/wp-content/uploads/2014/04/dl_hacks2015-04-21-iwasawa1.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "import six\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chainer exampleに付属のdata.pyをimportする. mnistのダウンロードのため\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpu_flag = 0\n",
    "if gpu_flag >= 0:\n",
    "    cuda.check_cuda_available()\n",
    "xp = cuda.cupy if gpu_flag >= 0 else np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTデータのダウンロードといくつかプロットして確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = data.load_mnist_data()\n",
    "# 70,000の手書き数字データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 手書き数字データを描画する関数\n",
    "def draw_digit(data):\n",
    "    size = 28\n",
    "    plt.figure(figsize=(2, 2))\n",
    "\n",
    "    X, Y = np.meshgrid(range(size),range(size))\n",
    "    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix\n",
    "    Z = Z[::-1,:]             # flip vertical\n",
    "    plt.xlim(0,27)\n",
    "    plt.ylim(0,27)\n",
    "    plt.pcolor(X, Y, Z)\n",
    "    plt.gray()\n",
    "    plt.tick_params(labelbottom=\"off\")\n",
    "    plt.tick_params(labelleft=\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 複数の画像配列になっているdata列を描画する\n",
    "def draw_digit2(data):\n",
    "    size = 28\n",
    "    n = data.shape[0]\n",
    "    plt.figure(figsize=(n, 2))\n",
    "    cnt = 1\n",
    "    for idx in np.arange(n):\n",
    "        plt.subplot(1, n, cnt)\n",
    "        X, Y = np.meshgrid(range(size),range(size))\n",
    "        Z = data[idx].reshape(size,size)   # convert from vector to 28x28 matrix\n",
    "        Z = Z[::-1,:]             # flip vertical\n",
    "        plt.xlim(0,27)\n",
    "        plt.ylim(0,27)\n",
    "        plt.pcolor(X, Y, Z)\n",
    "        plt.gray()\n",
    "        plt.tick_params(labelbottom=\"off\")\n",
    "        plt.tick_params(labelleft=\"off\")\n",
    "        cnt+=1\n",
    "    plt.show()\n",
    "\n",
    "# いくつかプロット\n",
    "def draw_digit_multi(data, n=10):\n",
    "    # サンプラー\n",
    "    indexes = np.random.permutation( len(data) )[:n]\n",
    "    # \n",
    "    size = 28\n",
    "    plt.figure(figsize=(15, 2))\n",
    "    # plot\n",
    "    cnt = 1\n",
    "    for idx in indexes:\n",
    "        plt.subplot(1, n, cnt)\n",
    "        X, Y = np.meshgrid(range(size),range(size))\n",
    "        Z = data[idx].reshape(size,size)   # convert from vector to 28x28 matrix\n",
    "        Z = Z[::-1,:]\n",
    "        plt.xlim(0, size-1)\n",
    "        plt.ylim(0, size-1)\n",
    "        plt.pcolor(X, Y, Z)\n",
    "        plt.gray()\n",
    "        plt.tick_params(labelbottom=\"off\")\n",
    "        plt.tick_params(labelleft=\"off\")\n",
    "        cnt+=1\n",
    "    plt.show()\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAB+CAYAAADBYkvpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAAGwmAIAAACAAP4PMQBIyfBz1vcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d661a1190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : \n",
      "[2 4 1 8 2 1 7 7 8 1]\n"
     ]
    }
   ],
   "source": [
    "idxs = draw_digit_multi(mnist['data'])\n",
    "print \"labels : \"\n",
    "print mnist['target'][idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 100 # ミニバッチのサイズ\n",
    "n_epoch = 500     # epoch数\n",
    "n_latent = 100   # 潜在変数の次元(DCGANで言うところのプライヤーベクトルの次元)\n",
    "\n",
    "# Optimizer\n",
    "al = 0.001\n",
    "b1 = 0.9\n",
    "b2 = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理\n",
    "* [0,1]の範囲に正規化\n",
    "* float32, int32に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 正規化\n",
    "mnist['data'] = mnist['data'].astype(np.float32)\n",
    "mnist['data'] /= 255\n",
    "mnist['target'] = mnist['target'].astype(np.int32)\n",
    "# 訓練データとテストデータに分割\n",
    "N = 60000\n",
    "x_train, x_test = np.split(mnist['data'],   [N])\n",
    "y_train, y_test = np.split(mnist['target'], [N])\n",
    "N_test = y_test.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義\n",
    "* 公式exampleではnet.pyとして別ファイルになっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 変分誤差を計算するためにKLダイバージェンスの関数をimportしておく\n",
    "from chainer.functions.loss.vae import gaussian_kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VAE(chainer.Chain):\n",
    "    \"\"\"Variational AutoEncoder\"\"\"\n",
    "    def __init__(self, n_in, n_latent, n_h):\n",
    "        super(VAE, self).__init__(\n",
    "            ## ネットワーク構造の定義\n",
    "            # encoder\n",
    "            le1=L.Linear(n_in, n_h),\n",
    "            le2_mu=L.Linear(n_h, n_latent),\n",
    "            le2_ln_var=L.Linear(n_h, n_latent),\n",
    "            # decoder\n",
    "            ld1=L.Linear(n_latent, n_h),\n",
    "            ld2=L.Linear(n_h, n_in),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, sigmoid=True):\n",
    "        \"\"\"AutoEncoder\"\"\"\n",
    "        # 下記、encodeとdecodeの中身をこの中に書いても良いがencodeとｄｅｃｏｄｅは他でも使うので再利用性を高めるために\n",
    "        return self.decode(self.encode(x)[0], sigmoid)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # 推論モデル, 中間表現のベクトルqを学習\n",
    "        h1 = F.tanh(self.le1(x))\n",
    "        mu = self.le2_mu(h1)\n",
    "        ln_var = self.le2_ln_var(h1)  # log(sigma**2)\n",
    "        return mu, ln_var\n",
    "\n",
    "    def decode(self, z, sigmoid=True):\n",
    "        # 中間表現ベクトルqを入力として(z), 画像を生成\n",
    "        h1 = F.tanh(self.ld1(z))\n",
    "        h2 = self.ld2(h1)\n",
    "        if sigmoid:\n",
    "            return F.sigmoid(h2)\n",
    "        else:\n",
    "            return h2\n",
    "\n",
    "    def get_loss_func(self, C=1.0, k=1, train=True):\n",
    "        \"\"\"Get loss function of VAE.\n",
    "        The loss value is equal to ELBO (Evidence Lower Bound)\n",
    "        multiplied by -1.\n",
    "        Args:\n",
    "            C (int): Usually this is 1.0. Can be changed to control the\n",
    "                second term of ELBO bound, which works as regularization.\n",
    "            k (int): Number of Monte Carlo samples used in encoded vector.\n",
    "            train (bool): If true loss_function is used for training.\n",
    "        \"\"\"\n",
    "        def lf(x):\n",
    "            mu, ln_var = self.encode(x)\n",
    "            batchsize = len(mu.data)\n",
    "            # reconstruction loss\n",
    "            rec_loss = 0\n",
    "            for l in six.moves.range(k):\n",
    "                z = F.gaussian(mu, ln_var)\n",
    "                rec_loss += F.bernoulli_nll(x, self.decode(z, sigmoid=False)) / (k * batchsize)\n",
    "            self.rec_loss = rec_loss\n",
    "            self.loss = self.rec_loss + C * gaussian_kl_divergence(mu, ln_var) / batchsize\n",
    "            return self.loss\n",
    "        return lf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAEモデルとOptimizerを設定して学習の準備をする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# モデルの設定\n",
    "model = VAE(784, n_latent, 500) # VAE(n_in, n_latent, n_h)  input=28*28=784\n",
    "if gpu_flag >= 0:\n",
    "    cuda.get_device(gpu_flag).use()\n",
    "    model.to_gpu()\n",
    "xp = np if gpu_flag < 0 else cuda.cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizerを定義する\n",
    "optimizer = optimizers.Adam(alpha=al, beta1=b1, beta2=b2)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch', 1)\n",
      "train mean loss=165.287917328, mean reconstruction loss=136.677214216\n",
      "('epoch', 2)\n",
      "train mean loss=130.985793864, mean reconstruction loss=99.2749990718\n",
      "('epoch', 3)\n",
      "train mean loss=122.389264984, mean reconstruction loss=91.3631133652\n",
      "('epoch', 4)\n",
      "train mean loss=118.562328211, mean reconstruction loss=87.9668908819\n",
      "('epoch', 5)\n",
      "train mean loss=115.812919909, mean reconstruction loss=85.9054897563\n",
      "('epoch', 6)\n",
      "train mean loss=113.836114998, mean reconstruction loss=84.4452307383\n",
      "('epoch', 7)\n",
      "train mean loss=112.327970606, mean reconstruction loss=83.3124522273\n",
      "('epoch', 8)\n",
      "train mean loss=111.060606308, mean reconstruction loss=82.3947008514\n",
      "('epoch', 9)\n",
      "train mean loss=110.068879992, mean reconstruction loss=81.645971489\n",
      "('epoch', 10)\n",
      "train mean loss=109.165320651, mean reconstruction loss=81.0185316722\n",
      "('epoch', 11)\n",
      "train mean loss=108.473997917, mean reconstruction loss=80.5153172175\n",
      "('epoch', 12)\n",
      "train mean loss=107.916353251, mean reconstruction loss=80.0914970271\n",
      "('epoch', 13)\n",
      "train mean loss=107.337661845, mean reconstruction loss=79.668326149\n",
      "('epoch', 14)\n",
      "train mean loss=106.862063154, mean reconstruction loss=79.3022932434\n",
      "('epoch', 15)\n",
      "train mean loss=106.492893651, mean reconstruction loss=79.0297217433\n",
      "('epoch', 16)\n",
      "train mean loss=106.215207303, mean reconstruction loss=78.7988897832\n",
      "('epoch', 17)\n",
      "train mean loss=105.928955956, mean reconstruction loss=78.5944923782\n",
      "('epoch', 18)\n",
      "train mean loss=105.634702784, mean reconstruction loss=78.3324007034\n",
      "('epoch', 19)\n",
      "train mean loss=105.433079236, mean reconstruction loss=78.1497168732\n",
      "('epoch', 20)\n",
      "train mean loss=105.226345838, mean reconstruction loss=78.0191790517\n",
      "('epoch', 21)\n",
      "train mean loss=105.022772433, mean reconstruction loss=77.8548270162\n",
      "('epoch', 22)\n",
      "train mean loss=104.799372177, mean reconstruction loss=77.69274189\n",
      "('epoch', 23)\n",
      "train mean loss=104.635759265, mean reconstruction loss=77.5627363841\n",
      "('epoch', 24)\n",
      "train mean loss=104.489718755, mean reconstruction loss=77.4412792079\n",
      "('epoch', 25)\n",
      "train mean loss=104.411626612, mean reconstruction loss=77.3635787964\n",
      "('epoch', 26)\n",
      "train mean loss=104.24971667, mean reconstruction loss=77.2542543666\n",
      "('epoch', 27)\n",
      "train mean loss=104.166548716, mean reconstruction loss=77.1871002197\n",
      "('epoch', 28)\n",
      "train mean loss=104.03489521, mean reconstruction loss=77.1051361338\n",
      "('epoch', 29)\n",
      "train mean loss=103.861316821, mean reconstruction loss=76.9971909968\n",
      "('epoch', 30)\n",
      "train mean loss=103.820854276, mean reconstruction loss=76.9215420024\n",
      "('epoch', 31)\n",
      "train mean loss=103.731376661, mean reconstruction loss=76.8514999771\n",
      "('epoch', 32)\n",
      "train mean loss=103.635840505, mean reconstruction loss=76.7870076243\n",
      "('epoch', 33)\n",
      "train mean loss=103.546464017, mean reconstruction loss=76.7196433767\n",
      "('epoch', 34)\n",
      "train mean loss=103.479001719, mean reconstruction loss=76.6917064031\n",
      "('epoch', 35)\n",
      "train mean loss=103.346308149, mean reconstruction loss=76.5855290604\n",
      "('epoch', 36)\n",
      "train mean loss=103.313502731, mean reconstruction loss=76.5586903\n",
      "('epoch', 37)\n",
      "train mean loss=103.246544329, mean reconstruction loss=76.504142952\n",
      "('epoch', 38)\n",
      "train mean loss=103.201836802, mean reconstruction loss=76.477876002\n",
      "('epoch', 39)\n",
      "train mean loss=103.117128563, mean reconstruction loss=76.409024442\n",
      "('epoch', 40)\n",
      "train mean loss=103.117309736, mean reconstruction loss=76.3735403061\n",
      "('epoch', 41)\n",
      "train mean loss=102.958158112, mean reconstruction loss=76.2966331228\n",
      "('epoch', 42)\n",
      "train mean loss=102.869842351, mean reconstruction loss=76.248758405\n",
      "('epoch', 43)\n",
      "train mean loss=102.903713888, mean reconstruction loss=76.251768748\n",
      "('epoch', 44)\n",
      "train mean loss=102.823471883, mean reconstruction loss=76.2093237432\n",
      "('epoch', 45)\n",
      "train mean loss=102.754798152, mean reconstruction loss=76.1606063716\n",
      "('epoch', 46)\n",
      "train mean loss=102.703097089, mean reconstruction loss=76.0996316655\n",
      "('epoch', 47)\n",
      "train mean loss=102.651677411, mean reconstruction loss=76.0750598526\n",
      "('epoch', 48)\n",
      "train mean loss=102.584076385, mean reconstruction loss=76.0249065145\n",
      "('epoch', 49)\n",
      "train mean loss=102.55148688, mean reconstruction loss=75.9889982859\n",
      "('epoch', 50)\n",
      "train mean loss=102.471033872, mean reconstruction loss=75.970269165\n",
      "('epoch', 51)\n",
      "train mean loss=102.5529518, mean reconstruction loss=76.0074994532\n",
      "('epoch', 52)\n",
      "train mean loss=102.415909309, mean reconstruction loss=75.92086483\n",
      "('epoch', 53)\n",
      "train mean loss=102.407489344, mean reconstruction loss=75.8974037806\n",
      "('epoch', 54)\n",
      "train mean loss=102.332706184, mean reconstruction loss=75.8430042013\n",
      "('epoch', 55)\n",
      "train mean loss=102.337789497, mean reconstruction loss=75.8490201314\n",
      "('epoch', 56)\n",
      "train mean loss=102.282293917, mean reconstruction loss=75.8149596786\n",
      "('epoch', 57)\n",
      "train mean loss=102.235915769, mean reconstruction loss=75.7983608882\n",
      "('epoch', 58)\n",
      "train mean loss=102.160790049, mean reconstruction loss=75.7452234395\n",
      "('epoch', 59)\n",
      "train mean loss=102.194308484, mean reconstruction loss=75.7909290441\n",
      "('epoch', 60)\n",
      "train mean loss=102.107315661, mean reconstruction loss=75.7397928492\n",
      "('epoch', 61)\n",
      "train mean loss=102.083875427, mean reconstruction loss=75.6823926417\n",
      "('epoch', 62)\n",
      "train mean loss=102.076852277, mean reconstruction loss=75.6982275136\n",
      "('epoch', 63)\n",
      "train mean loss=101.982260335, mean reconstruction loss=75.6382589213\n",
      "('epoch', 64)\n",
      "train mean loss=102.035842336, mean reconstruction loss=75.6772197596\n",
      "('epoch', 65)\n",
      "train mean loss=101.978849894, mean reconstruction loss=75.6397587967\n",
      "('epoch', 66)\n",
      "train mean loss=101.837255745, mean reconstruction loss=75.5418498993\n",
      "('epoch', 67)\n",
      "train mean loss=101.902522977, mean reconstruction loss=75.5876901754\n",
      "('epoch', 68)\n",
      "train mean loss=101.882796199, mean reconstruction loss=75.5701404572\n",
      "('epoch', 69)\n",
      "train mean loss=101.884267845, mean reconstruction loss=75.5614226151\n",
      "('epoch', 70)\n",
      "train mean loss=101.771315091, mean reconstruction loss=75.5210067368\n",
      "('epoch', 71)\n",
      "train mean loss=101.775267334, mean reconstruction loss=75.5098455429\n",
      "('epoch', 72)\n",
      "train mean loss=101.771238073, mean reconstruction loss=75.5119492594\n",
      "('epoch', 73)\n",
      "train mean loss=101.740184669, mean reconstruction loss=75.5057349141\n",
      "('epoch', 74)\n",
      "train mean loss=101.691994578, mean reconstruction loss=75.4759898122\n",
      "('epoch', 75)\n",
      "train mean loss=101.671600596, mean reconstruction loss=75.4648470688\n",
      "('epoch', 76)\n",
      "train mean loss=101.600424232, mean reconstruction loss=75.4153672663\n",
      "('epoch', 77)\n",
      "train mean loss=101.599878273, mean reconstruction loss=75.4248160426\n",
      "('epoch', 78)\n",
      "train mean loss=101.571233431, mean reconstruction loss=75.4204334513\n",
      "('epoch', 79)\n",
      "train mean loss=101.563732363, mean reconstruction loss=75.4129102961\n",
      "('epoch', 80)\n",
      "train mean loss=101.540522232, mean reconstruction loss=75.372122256\n",
      "('epoch', 81)\n",
      "train mean loss=101.52453701, mean reconstruction loss=75.362821757\n",
      "('epoch', 82)\n",
      "train mean loss=101.53707564, mean reconstruction loss=75.3796324158\n",
      "('epoch', 83)\n",
      "train mean loss=101.488878441, mean reconstruction loss=75.3489016469\n",
      "('epoch', 84)\n",
      "train mean loss=101.377641424, mean reconstruction loss=75.2746503576\n",
      "('epoch', 85)\n",
      "train mean loss=101.411733907, mean reconstruction loss=75.3195094045\n",
      "('epoch', 86)\n",
      "train mean loss=101.426599642, mean reconstruction loss=75.3140240097\n",
      "('epoch', 87)\n",
      "train mean loss=101.375736364, mean reconstruction loss=75.2781484731\n",
      "('epoch', 88)\n",
      "train mean loss=101.367498461, mean reconstruction loss=75.2640642802\n",
      "('epoch', 89)\n",
      "train mean loss=101.249688848, mean reconstruction loss=75.22237559\n",
      "('epoch', 90)\n",
      "train mean loss=101.304272092, mean reconstruction loss=75.2308349609\n",
      "('epoch', 91)\n",
      "train mean loss=101.294920464, mean reconstruction loss=75.2417453512\n",
      "('epoch', 92)\n",
      "train mean loss=101.227235718, mean reconstruction loss=75.2059102631\n",
      "('epoch', 93)\n",
      "train mean loss=101.231522153, mean reconstruction loss=75.1896549861\n",
      "('epoch', 94)\n",
      "train mean loss=101.195024223, mean reconstruction loss=75.1764987183\n",
      "('epoch', 95)\n",
      "train mean loss=101.190057551, mean reconstruction loss=75.15821462\n",
      "('epoch', 96)\n",
      "train mean loss=101.195076574, mean reconstruction loss=75.1780180232\n",
      "('epoch', 97)\n",
      "train mean loss=101.148797862, mean reconstruction loss=75.1235945511\n",
      "('epoch', 98)\n",
      "train mean loss=101.14482947, mean reconstruction loss=75.1503465271\n",
      "('epoch', 99)\n",
      "train mean loss=101.120899467, mean reconstruction loss=75.1092650731\n",
      "('epoch', 100)\n",
      "train mean loss=101.0928685, mean reconstruction loss=75.126224734\n",
      "('epoch', 101)\n",
      "train mean loss=101.062614136, mean reconstruction loss=75.0835561879\n",
      "('epoch', 102)\n",
      "train mean loss=101.052947489, mean reconstruction loss=75.0885018667\n",
      "('epoch', 103)\n",
      "train mean loss=101.068972867, mean reconstruction loss=75.1025670751\n",
      "('epoch', 104)\n",
      "train mean loss=101.063254369, mean reconstruction loss=75.0913213476\n",
      "('epoch', 105)\n",
      "train mean loss=101.046670926, mean reconstruction loss=75.0361296209\n",
      "('epoch', 106)\n",
      "train mean loss=101.031285222, mean reconstruction loss=75.0603995895\n",
      "('epoch', 107)\n",
      "train mean loss=100.937782033, mean reconstruction loss=75.0283434168\n",
      "('epoch', 108)\n",
      "train mean loss=101.015649478, mean reconstruction loss=75.0365478388\n",
      "('epoch', 109)\n",
      "train mean loss=100.922605476, mean reconstruction loss=75.0009087626\n",
      "('epoch', 110)\n",
      "train mean loss=100.963793666, mean reconstruction loss=75.0260520299\n",
      "('epoch', 111)\n",
      "train mean loss=100.917029775, mean reconstruction loss=74.9827775955\n",
      "('epoch', 112)\n",
      "train mean loss=100.966222076, mean reconstruction loss=75.0055094147\n",
      "('epoch', 113)\n",
      "train mean loss=100.876936493, mean reconstruction loss=74.9809009043\n",
      "('epoch', 114)\n",
      "train mean loss=100.833935369, mean reconstruction loss=74.9537866592\n",
      "('epoch', 115)\n",
      "train mean loss=100.88186868, mean reconstruction loss=74.9972101212\n",
      "('epoch', 116)\n",
      "train mean loss=100.820962817, mean reconstruction loss=74.9443899155\n",
      "('epoch', 117)\n",
      "train mean loss=100.831066144, mean reconstruction loss=74.9350158819\n",
      "('epoch', 118)\n",
      "train mean loss=100.819469541, mean reconstruction loss=74.9251143773\n",
      "('epoch', 119)\n",
      "train mean loss=100.799017919, mean reconstruction loss=74.9260785294\n",
      "('epoch', 120)\n",
      "train mean loss=100.812872645, mean reconstruction loss=74.9044922892\n",
      "('epoch', 121)\n",
      "train mean loss=100.743601519, mean reconstruction loss=74.8921780523\n",
      "('epoch', 122)\n",
      "train mean loss=100.786925087, mean reconstruction loss=74.9211003621\n",
      "('epoch', 123)\n",
      "train mean loss=100.741096433, mean reconstruction loss=74.9069081624\n",
      "('epoch', 124)\n",
      "train mean loss=100.746619797, mean reconstruction loss=74.8695246251\n",
      "('epoch', 125)\n",
      "train mean loss=100.750959384, mean reconstruction loss=74.8648703003\n",
      "('epoch', 126)\n",
      "train mean loss=100.668366203, mean reconstruction loss=74.8338665263\n",
      "('epoch', 127)\n",
      "train mean loss=100.695728607, mean reconstruction loss=74.8808740997\n",
      "('epoch', 128)\n",
      "train mean loss=100.660349083, mean reconstruction loss=74.8490061569\n",
      "('epoch', 129)\n",
      "train mean loss=100.64460172, mean reconstruction loss=74.8269294993\n",
      "('epoch', 130)\n",
      "train mean loss=100.668674189, mean reconstruction loss=74.852574234\n",
      "('epoch', 131)\n",
      "train mean loss=100.63554985, mean reconstruction loss=74.8406775284\n",
      "('epoch', 132)\n",
      "train mean loss=100.679302279, mean reconstruction loss=74.8351392365\n",
      "('epoch', 133)\n",
      "train mean loss=100.635078239, mean reconstruction loss=74.8425009918\n",
      "('epoch', 134)\n",
      "train mean loss=100.606737607, mean reconstruction loss=74.8256369019\n",
      "('epoch', 135)\n",
      "train mean loss=100.564403013, mean reconstruction loss=74.7821992111\n",
      "('epoch', 136)\n",
      "train mean loss=100.595563545, mean reconstruction loss=74.7987868118\n",
      "('epoch', 137)\n",
      "train mean loss=100.574963671, mean reconstruction loss=74.7932198207\n",
      "('epoch', 138)\n",
      "train mean loss=100.548691661, mean reconstruction loss=74.7716185125\n",
      "('epoch', 139)\n",
      "train mean loss=100.537806282, mean reconstruction loss=74.7799951808\n",
      "('epoch', 140)\n",
      "train mean loss=100.520457484, mean reconstruction loss=74.7700762049\n",
      "('epoch', 141)\n",
      "train mean loss=100.525128746, mean reconstruction loss=74.780064532\n",
      "('epoch', 142)\n",
      "train mean loss=100.508417778, mean reconstruction loss=74.76146197\n",
      "('epoch', 143)\n",
      "train mean loss=100.500090472, mean reconstruction loss=74.7474214808\n",
      "('epoch', 144)\n",
      "train mean loss=100.465545731, mean reconstruction loss=74.7344199117\n",
      "('epoch', 145)\n",
      "train mean loss=100.442097143, mean reconstruction loss=74.7366112645\n",
      "('epoch', 146)\n",
      "train mean loss=100.476245219, mean reconstruction loss=74.7350018056\n",
      "('epoch', 147)\n",
      "train mean loss=100.456345545, mean reconstruction loss=74.7135953903\n",
      "('epoch', 148)\n",
      "train mean loss=100.447169342, mean reconstruction loss=74.7466026688\n",
      "('epoch', 149)\n",
      "train mean loss=100.410217896, mean reconstruction loss=74.7106020864\n",
      "('epoch', 150)\n",
      "train mean loss=100.422599271, mean reconstruction loss=74.7107017771\n",
      "('epoch', 151)\n",
      "train mean loss=100.422575111, mean reconstruction loss=74.7073171488\n",
      "('epoch', 152)\n",
      "train mean loss=100.333535995, mean reconstruction loss=74.6702083842\n",
      "('epoch', 153)\n",
      "train mean loss=100.431415367, mean reconstruction loss=74.7051564026\n",
      "('epoch', 154)\n",
      "train mean loss=100.41717322, mean reconstruction loss=74.7218187332\n",
      "('epoch', 155)\n",
      "train mean loss=100.386858063, mean reconstruction loss=74.6969690196\n",
      "('epoch', 156)\n",
      "train mean loss=100.340286013, mean reconstruction loss=74.6596552149\n",
      "('epoch', 157)\n",
      "train mean loss=100.369735463, mean reconstruction loss=74.6896815745\n",
      "('epoch', 158)\n",
      "train mean loss=100.365118446, mean reconstruction loss=74.6858765793\n",
      "('epoch', 159)\n",
      "train mean loss=100.329229329, mean reconstruction loss=74.6458916855\n",
      "('epoch', 160)\n",
      "train mean loss=100.281251945, mean reconstruction loss=74.6219009399\n",
      "('epoch', 161)\n",
      "train mean loss=100.348293126, mean reconstruction loss=74.6640611521\n",
      "('epoch', 162)\n",
      "train mean loss=100.309876874, mean reconstruction loss=74.6687703323\n",
      "('epoch', 163)\n",
      "train mean loss=100.280386314, mean reconstruction loss=74.6423441696\n",
      "('epoch', 164)\n",
      "train mean loss=100.296012662, mean reconstruction loss=74.6378202947\n",
      "('epoch', 165)\n",
      "train mean loss=100.293817444, mean reconstruction loss=74.6517218272\n",
      "('epoch', 166)\n",
      "train mean loss=100.294632861, mean reconstruction loss=74.6534776815\n",
      "('epoch', 167)\n",
      "train mean loss=100.270129522, mean reconstruction loss=74.6348872248\n",
      "('epoch', 168)\n",
      "train mean loss=100.244766172, mean reconstruction loss=74.6231207911\n",
      "('epoch', 169)\n",
      "train mean loss=100.282457479, mean reconstruction loss=74.6362831497\n",
      "('epoch', 170)\n",
      "train mean loss=100.225192019, mean reconstruction loss=74.6152728526\n",
      "('epoch', 171)\n",
      "train mean loss=100.24883522, mean reconstruction loss=74.6444860077\n",
      "('epoch', 172)\n",
      "train mean loss=100.231976191, mean reconstruction loss=74.6022914632\n",
      "('epoch', 173)\n",
      "train mean loss=100.167231356, mean reconstruction loss=74.5999551137\n",
      "('epoch', 174)\n",
      "train mean loss=100.22178538, mean reconstruction loss=74.6173206329\n",
      "('epoch', 175)\n",
      "train mean loss=100.184336573, mean reconstruction loss=74.6066375224\n",
      "('epoch', 176)\n",
      "train mean loss=100.169277992, mean reconstruction loss=74.5833406703\n",
      "('epoch', 177)\n",
      "train mean loss=100.146978544, mean reconstruction loss=74.5770447667\n",
      "('epoch', 178)\n",
      "train mean loss=100.194546445, mean reconstruction loss=74.6122319667\n",
      "('epoch', 179)\n",
      "train mean loss=100.153932203, mean reconstruction loss=74.5862595113\n",
      "('epoch', 180)\n",
      "train mean loss=100.168480377, mean reconstruction loss=74.5939895757\n",
      "('epoch', 181)\n",
      "train mean loss=100.153352242, mean reconstruction loss=74.5852680969\n",
      "('epoch', 182)\n",
      "train mean loss=100.145841331, mean reconstruction loss=74.5863439941\n",
      "('epoch', 183)\n",
      "train mean loss=100.114444071, mean reconstruction loss=74.565495224\n",
      "('epoch', 184)\n",
      "train mean loss=100.134614042, mean reconstruction loss=74.5415488307\n",
      "('epoch', 185)\n",
      "train mean loss=100.151472041, mean reconstruction loss=74.5902602132\n",
      "('epoch', 186)\n",
      "train mean loss=100.110918172, mean reconstruction loss=74.5797818247\n",
      "('epoch', 187)\n",
      "train mean loss=100.115762138, mean reconstruction loss=74.5660020447\n",
      "('epoch', 188)\n",
      "train mean loss=100.121492577, mean reconstruction loss=74.6139922969\n",
      "('epoch', 189)\n",
      "train mean loss=100.068458811, mean reconstruction loss=74.5447653961\n",
      "('epoch', 190)\n",
      "train mean loss=100.044616369, mean reconstruction loss=74.5286413066\n",
      "('epoch', 191)\n",
      "train mean loss=100.076652044, mean reconstruction loss=74.5577321625\n",
      "('epoch', 192)\n",
      "train mean loss=100.048772634, mean reconstruction loss=74.5395846939\n",
      "('epoch', 193)\n",
      "train mean loss=100.06322286, mean reconstruction loss=74.5230722427\n",
      "('epoch', 194)\n",
      "train mean loss=100.048161481, mean reconstruction loss=74.5414714432\n",
      "('epoch', 195)\n",
      "train mean loss=100.016500257, mean reconstruction loss=74.5123449071\n",
      "('epoch', 196)\n",
      "train mean loss=100.074890671, mean reconstruction loss=74.5376634598\n",
      "('epoch', 197)\n",
      "train mean loss=100.010931422, mean reconstruction loss=74.522789065\n",
      "('epoch', 198)\n",
      "train mean loss=100.018617121, mean reconstruction loss=74.5239134216\n",
      "('epoch', 199)\n",
      "train mean loss=100.019092547, mean reconstruction loss=74.5272632345\n",
      "('epoch', 200)\n",
      "train mean loss=100.03269118, mean reconstruction loss=74.5433331807\n",
      "('epoch', 201)\n",
      "train mean loss=99.9360690816, mean reconstruction loss=74.4747142283\n",
      "('epoch', 202)\n",
      "train mean loss=99.9992304103, mean reconstruction loss=74.517048556\n",
      "('epoch', 203)\n",
      "train mean loss=99.956642367, mean reconstruction loss=74.4983077113\n",
      "('epoch', 204)\n",
      "train mean loss=100.006388919, mean reconstruction loss=74.5292850876\n",
      "('epoch', 205)\n",
      "train mean loss=99.9328546397, mean reconstruction loss=74.4830095164\n",
      "('epoch', 206)\n",
      "train mean loss=99.9449490865, mean reconstruction loss=74.510436554\n",
      "('epoch', 207)\n",
      "train mean loss=99.9675864029, mean reconstruction loss=74.5352741496\n",
      "('epoch', 208)\n",
      "train mean loss=99.95170283, mean reconstruction loss=74.4919058609\n",
      "('epoch', 209)\n",
      "train mean loss=99.9227635066, mean reconstruction loss=74.4797605387\n",
      "('epoch', 210)\n",
      "train mean loss=99.8981818136, mean reconstruction loss=74.475026741\n",
      "('epoch', 211)\n",
      "train mean loss=99.9731199773, mean reconstruction loss=74.5081424459\n",
      "('epoch', 212)\n",
      "train mean loss=99.9634124247, mean reconstruction loss=74.5203013102\n",
      "('epoch', 213)\n",
      "train mean loss=99.9279520035, mean reconstruction loss=74.4911766052\n",
      "('epoch', 214)\n",
      "train mean loss=99.9012251663, mean reconstruction loss=74.4906606038\n",
      "('epoch', 215)\n",
      "train mean loss=99.9022594452, mean reconstruction loss=74.4744422404\n",
      "('epoch', 216)\n",
      "train mean loss=99.8143699265, mean reconstruction loss=74.4243769455\n",
      "('epoch', 217)\n",
      "train mean loss=99.8621955617, mean reconstruction loss=74.4290405401\n",
      "('epoch', 218)\n",
      "train mean loss=99.8890740585, mean reconstruction loss=74.4812904358\n",
      "('epoch', 219)\n",
      "train mean loss=99.8498878479, mean reconstruction loss=74.4584527715\n",
      "('epoch', 220)\n",
      "train mean loss=99.8552519353, mean reconstruction loss=74.4614437612\n",
      "('epoch', 221)\n",
      "train mean loss=99.8681562169, mean reconstruction loss=74.472598114\n",
      "('epoch', 222)\n",
      "train mean loss=99.891273969, mean reconstruction loss=74.457024854\n",
      "('epoch', 223)\n",
      "train mean loss=99.9012270355, mean reconstruction loss=74.5091706848\n",
      "('epoch', 224)\n",
      "train mean loss=99.8579723358, mean reconstruction loss=74.4705597432\n",
      "('epoch', 225)\n",
      "train mean loss=99.794915568, mean reconstruction loss=74.4322860845\n",
      "('epoch', 226)\n",
      "train mean loss=99.8569630051, mean reconstruction loss=74.4702802277\n",
      "('epoch', 227)\n",
      "train mean loss=99.8251932271, mean reconstruction loss=74.4763214111\n",
      "('epoch', 228)\n",
      "train mean loss=99.8067082087, mean reconstruction loss=74.4609839884\n",
      "('epoch', 229)\n",
      "train mean loss=99.7829167302, mean reconstruction loss=74.4531646729\n",
      "('epoch', 230)\n",
      "train mean loss=99.8206711578, mean reconstruction loss=74.4606383387\n",
      "('epoch', 231)\n",
      "train mean loss=99.8370748266, mean reconstruction loss=74.4751650492\n",
      "('epoch', 232)\n",
      "train mean loss=99.7856679408, mean reconstruction loss=74.4285311508\n",
      "('epoch', 233)\n",
      "train mean loss=99.7776107152, mean reconstruction loss=74.4171243159\n",
      "('epoch', 234)\n",
      "train mean loss=99.8177699407, mean reconstruction loss=74.4459521612\n",
      "('epoch', 235)\n",
      "train mean loss=99.770144577, mean reconstruction loss=74.4261186091\n",
      "('epoch', 236)\n",
      "train mean loss=99.7993799845, mean reconstruction loss=74.4341496023\n",
      "('epoch', 237)\n",
      "train mean loss=99.7743546041, mean reconstruction loss=74.4410463715\n",
      "('epoch', 238)\n",
      "train mean loss=99.7551903661, mean reconstruction loss=74.425233167\n",
      "('epoch', 239)\n",
      "train mean loss=99.7844875209, mean reconstruction loss=74.4591176987\n",
      "('epoch', 240)\n",
      "train mean loss=99.7897846985, mean reconstruction loss=74.4384277089\n",
      "('epoch', 241)\n",
      "train mean loss=99.7401960373, mean reconstruction loss=74.4198532232\n",
      "('epoch', 242)\n",
      "train mean loss=99.7541829427, mean reconstruction loss=74.4330183283\n",
      "('epoch', 243)\n",
      "train mean loss=99.7338532384, mean reconstruction loss=74.4219613012\n",
      "('epoch', 244)\n",
      "train mean loss=99.7351404572, mean reconstruction loss=74.4316451263\n",
      "('epoch', 245)\n",
      "train mean loss=99.7379832077, mean reconstruction loss=74.4248509216\n",
      "('epoch', 246)\n",
      "train mean loss=99.751091156, mean reconstruction loss=74.4388235219\n",
      "('epoch', 247)\n",
      "train mean loss=99.7128181458, mean reconstruction loss=74.4102872594\n",
      "('epoch', 248)\n",
      "train mean loss=99.7222422918, mean reconstruction loss=74.4214764531\n",
      "('epoch', 249)\n",
      "train mean loss=99.7289016342, mean reconstruction loss=74.4246900431\n",
      "('epoch', 250)\n",
      "train mean loss=99.7343398031, mean reconstruction loss=74.4215652212\n",
      "('epoch', 251)\n",
      "train mean loss=99.7103576787, mean reconstruction loss=74.4208970133\n",
      "('epoch', 252)\n",
      "train mean loss=99.6960681915, mean reconstruction loss=74.4106164042\n",
      "('epoch', 253)\n",
      "train mean loss=99.7157974625, mean reconstruction loss=74.4262972514\n",
      "('epoch', 254)\n",
      "train mean loss=99.6553270976, mean reconstruction loss=74.3972884115\n",
      "('epoch', 255)\n",
      "train mean loss=99.6828106944, mean reconstruction loss=74.4278019206\n",
      "('epoch', 256)\n",
      "train mean loss=99.6645144908, mean reconstruction loss=74.4143508657\n",
      "('epoch', 257)\n",
      "train mean loss=99.6273296356, mean reconstruction loss=74.3688837306\n",
      "('epoch', 258)\n",
      "train mean loss=99.6492600886, mean reconstruction loss=74.4050782776\n",
      "('epoch', 259)\n",
      "train mean loss=99.6286625544, mean reconstruction loss=74.3748930359\n",
      "('epoch', 260)\n",
      "train mean loss=99.6747190221, mean reconstruction loss=74.4148708471\n",
      "('epoch', 261)\n",
      "train mean loss=99.6740343094, mean reconstruction loss=74.410387942\n",
      "('epoch', 262)\n",
      "train mean loss=99.6168784332, mean reconstruction loss=74.3825163523\n",
      "('epoch', 263)\n",
      "train mean loss=99.6175839488, mean reconstruction loss=74.379617068\n",
      "('epoch', 264)\n",
      "train mean loss=99.6069533157, mean reconstruction loss=74.3990564473\n",
      "('epoch', 265)\n",
      "train mean loss=99.6505365372, mean reconstruction loss=74.4089717992\n",
      "('epoch', 266)\n",
      "train mean loss=99.6258315531, mean reconstruction loss=74.4018911362\n",
      "('epoch', 267)\n",
      "train mean loss=99.6018039068, mean reconstruction loss=74.3783112208\n",
      "('epoch', 268)\n",
      "train mean loss=99.5803574626, mean reconstruction loss=74.3791445287\n",
      "('epoch', 269)\n",
      "train mean loss=99.6319635264, mean reconstruction loss=74.4323012288\n",
      "('epoch', 270)\n",
      "train mean loss=99.6208703359, mean reconstruction loss=74.4026304499\n",
      "('epoch', 271)\n",
      "train mean loss=99.5424933624, mean reconstruction loss=74.3772908274\n",
      "('epoch', 272)\n",
      "train mean loss=99.615635376, mean reconstruction loss=74.4054457982\n",
      "('epoch', 273)\n",
      "train mean loss=99.5416710536, mean reconstruction loss=74.3753549703\n",
      "('epoch', 274)\n",
      "train mean loss=99.6059154256, mean reconstruction loss=74.4059322484\n",
      "('epoch', 275)\n",
      "train mean loss=99.5839876429, mean reconstruction loss=74.4091315333\n",
      "('epoch', 276)\n",
      "train mean loss=99.5562046687, mean reconstruction loss=74.3657833862\n",
      "('epoch', 277)\n",
      "train mean loss=99.5927339681, mean reconstruction loss=74.3861266963\n",
      "('epoch', 278)\n",
      "train mean loss=99.5361842855, mean reconstruction loss=74.3567050298\n",
      "('epoch', 279)\n",
      "train mean loss=99.5306436157, mean reconstruction loss=74.3818082428\n",
      "('epoch', 280)\n",
      "train mean loss=99.5154489517, mean reconstruction loss=74.3716566722\n",
      "('epoch', 281)\n",
      "train mean loss=99.5507645543, mean reconstruction loss=74.3747577286\n",
      "('epoch', 282)\n",
      "train mean loss=99.5787947845, mean reconstruction loss=74.4122736994\n",
      "('epoch', 283)\n",
      "train mean loss=99.5767316437, mean reconstruction loss=74.4041176224\n",
      "('epoch', 284)\n",
      "train mean loss=99.4866323853, mean reconstruction loss=74.3434700139\n",
      "('epoch', 285)\n",
      "train mean loss=99.5058097585, mean reconstruction loss=74.3902240117\n",
      "('epoch', 286)\n",
      "train mean loss=99.5029357147, mean reconstruction loss=74.3760243098\n",
      "('epoch', 287)\n",
      "train mean loss=99.5526843389, mean reconstruction loss=74.3981313833\n",
      "('epoch', 288)\n",
      "train mean loss=99.4723827489, mean reconstruction loss=74.3408918254\n",
      "('epoch', 289)\n",
      "train mean loss=99.5059080124, mean reconstruction loss=74.3874181239\n",
      "('epoch', 290)\n",
      "train mean loss=99.5110249964, mean reconstruction loss=74.394991099\n",
      "('epoch', 291)\n",
      "train mean loss=99.4785583496, mean reconstruction loss=74.3627072779\n",
      "('epoch', 292)\n",
      "train mean loss=99.5283212407, mean reconstruction loss=74.4132316081\n",
      "('epoch', 293)\n",
      "train mean loss=99.4877135213, mean reconstruction loss=74.3776556142\n",
      "('epoch', 294)\n",
      "train mean loss=99.4710692342, mean reconstruction loss=74.3640681585\n",
      "('epoch', 295)\n",
      "train mean loss=99.4493003591, mean reconstruction loss=74.3261903127\n",
      "('epoch', 296)\n",
      "train mean loss=99.5050550715, mean reconstruction loss=74.4032822164\n",
      "('epoch', 297)\n",
      "train mean loss=99.4765553538, mean reconstruction loss=74.385572052\n",
      "('epoch', 298)\n",
      "train mean loss=99.4981056086, mean reconstruction loss=74.3828901418\n",
      "('epoch', 299)\n",
      "train mean loss=99.4512813441, mean reconstruction loss=74.386565272\n",
      "('epoch', 300)\n",
      "train mean loss=99.4642130025, mean reconstruction loss=74.3964606349\n",
      "('epoch', 301)\n",
      "train mean loss=99.4500126266, mean reconstruction loss=74.3699122238\n",
      "('epoch', 302)\n",
      "train mean loss=99.4651854324, mean reconstruction loss=74.3826856613\n",
      "('epoch', 303)\n",
      "train mean loss=99.4118957265, mean reconstruction loss=74.3645808919\n",
      "('epoch', 304)\n",
      "train mean loss=99.4467019272, mean reconstruction loss=74.3766533534\n",
      "('epoch', 305)\n",
      "train mean loss=99.4193034363, mean reconstruction loss=74.3647818883\n",
      "('epoch', 306)\n",
      "train mean loss=99.4246822866, mean reconstruction loss=74.3495069631\n",
      "('epoch', 307)\n",
      "train mean loss=99.4564154816, mean reconstruction loss=74.3929378764\n",
      "('epoch', 308)\n",
      "train mean loss=99.4061079661, mean reconstruction loss=74.3558125305\n",
      "('epoch', 309)\n",
      "train mean loss=99.4229029465, mean reconstruction loss=74.3695547104\n",
      "('epoch', 310)\n",
      "train mean loss=99.385429128, mean reconstruction loss=74.3442938105\n",
      "('epoch', 311)\n",
      "train mean loss=99.4111482366, mean reconstruction loss=74.35753287\n",
      "('epoch', 312)\n",
      "train mean loss=99.3712116369, mean reconstruction loss=74.3583506648\n",
      "('epoch', 313)\n",
      "train mean loss=99.4004840851, mean reconstruction loss=74.3727741623\n",
      "('epoch', 314)\n",
      "train mean loss=99.3854403178, mean reconstruction loss=74.3582633718\n",
      "('epoch', 315)\n",
      "train mean loss=99.3375433858, mean reconstruction loss=74.3313853836\n",
      "('epoch', 316)\n",
      "train mean loss=99.4231166458, mean reconstruction loss=74.3850653839\n",
      "('epoch', 317)\n",
      "train mean loss=99.3867661667, mean reconstruction loss=74.3796214167\n",
      "('epoch', 318)\n",
      "train mean loss=99.3142057037, mean reconstruction loss=74.307364006\n",
      "('epoch', 319)\n",
      "train mean loss=99.373680865, mean reconstruction loss=74.3404069138\n",
      "('epoch', 320)\n",
      "train mean loss=99.3534897232, mean reconstruction loss=74.345823199\n",
      "('epoch', 321)\n",
      "train mean loss=99.3647632599, mean reconstruction loss=74.3532158279\n",
      "('epoch', 322)\n",
      "train mean loss=99.3353105545, mean reconstruction loss=74.3347081629\n",
      "('epoch', 323)\n",
      "train mean loss=99.3542458216, mean reconstruction loss=74.346350975\n",
      "('epoch', 324)\n",
      "train mean loss=99.3568441645, mean reconstruction loss=74.3581932449\n",
      "('epoch', 325)\n",
      "train mean loss=99.3502768071, mean reconstruction loss=74.3768329239\n",
      "('epoch', 326)\n",
      "train mean loss=99.3224454117, mean reconstruction loss=74.3392613093\n",
      "('epoch', 327)\n",
      "train mean loss=99.2630121231, mean reconstruction loss=74.2874865214\n",
      "('epoch', 328)\n",
      "train mean loss=99.3066773987, mean reconstruction loss=74.3050367228\n",
      "('epoch', 329)\n",
      "train mean loss=99.3229297002, mean reconstruction loss=74.3408242671\n",
      "('epoch', 330)\n",
      "train mean loss=99.3234161758, mean reconstruction loss=74.3222399267\n",
      "('epoch', 331)\n",
      "train mean loss=99.3418955612, mean reconstruction loss=74.3479629517\n",
      "('epoch', 332)\n",
      "train mean loss=99.2950099055, mean reconstruction loss=74.325106074\n",
      "('epoch', 333)\n",
      "train mean loss=99.3205032349, mean reconstruction loss=74.3328564707\n",
      "('epoch', 334)\n",
      "train mean loss=99.2705061595, mean reconstruction loss=74.3127189636\n",
      "('epoch', 335)\n",
      "train mean loss=99.2770165761, mean reconstruction loss=74.3117655182\n",
      "('epoch', 336)\n",
      "train mean loss=99.2992391459, mean reconstruction loss=74.3387014262\n",
      "('epoch', 337)\n",
      "train mean loss=99.2773639679, mean reconstruction loss=74.302154007\n",
      "('epoch', 338)\n",
      "train mean loss=99.2254765574, mean reconstruction loss=74.2658195368\n",
      "('epoch', 339)\n",
      "train mean loss=99.283535614, mean reconstruction loss=74.3355415217\n",
      "('epoch', 340)\n",
      "train mean loss=99.2809819921, mean reconstruction loss=74.3219459661\n",
      "('epoch', 341)\n",
      "train mean loss=99.2517399724, mean reconstruction loss=74.3121922048\n",
      "('epoch', 342)\n",
      "train mean loss=99.2509799067, mean reconstruction loss=74.3097952143\n",
      "('epoch', 343)\n",
      "train mean loss=99.299922994, mean reconstruction loss=74.3385528692\n",
      "('epoch', 344)\n",
      "train mean loss=99.225435969, mean reconstruction loss=74.2843206914\n",
      "('epoch', 345)\n",
      "train mean loss=99.2782683055, mean reconstruction loss=74.3268383916\n",
      "('epoch', 346)\n",
      "train mean loss=99.205157903, mean reconstruction loss=74.2740778224\n",
      "('epoch', 347)\n",
      "train mean loss=99.2768848674, mean reconstruction loss=74.3261084112\n",
      "('epoch', 348)\n",
      "train mean loss=99.2203253174, mean reconstruction loss=74.2694252141\n",
      "('epoch', 349)\n",
      "train mean loss=99.2084535853, mean reconstruction loss=74.2961126582\n",
      "('epoch', 350)\n",
      "train mean loss=99.2373436483, mean reconstruction loss=74.296190389\n",
      "('epoch', 351)\n",
      "train mean loss=99.2611109797, mean reconstruction loss=74.2975874456\n",
      "('epoch', 352)\n",
      "train mean loss=99.2177920151, mean reconstruction loss=74.2788422648\n",
      "('epoch', 353)\n",
      "train mean loss=99.2264600754, mean reconstruction loss=74.3019388962\n",
      "('epoch', 354)\n",
      "train mean loss=99.1629001999, mean reconstruction loss=74.240375824\n",
      "('epoch', 355)\n",
      "train mean loss=99.2272220993, mean reconstruction loss=74.2955310186\n",
      "('epoch', 356)\n",
      "train mean loss=99.1874954478, mean reconstruction loss=74.2983540471\n",
      "('epoch', 357)\n",
      "train mean loss=99.2095541636, mean reconstruction loss=74.2804110463\n",
      "('epoch', 358)\n",
      "train mean loss=99.2279903666, mean reconstruction loss=74.2831502406\n",
      "('epoch', 359)\n",
      "train mean loss=99.1996091843, mean reconstruction loss=74.2802469254\n",
      "('epoch', 360)\n",
      "train mean loss=99.1759661865, mean reconstruction loss=74.2550406392\n",
      "('epoch', 361)\n",
      "train mean loss=99.1409972382, mean reconstruction loss=74.2528494517\n",
      "('epoch', 362)\n",
      "train mean loss=99.1768994268, mean reconstruction loss=74.2717024994\n",
      "('epoch', 363)\n",
      "train mean loss=99.1771881994, mean reconstruction loss=74.2569770177\n",
      "('epoch', 364)\n",
      "train mean loss=99.1730551783, mean reconstruction loss=74.2743571981\n",
      "('epoch', 365)\n",
      "train mean loss=99.1968326696, mean reconstruction loss=74.2841604614\n",
      "('epoch', 366)\n",
      "train mean loss=99.1568883006, mean reconstruction loss=74.2381297048\n",
      "('epoch', 367)\n",
      "train mean loss=99.1818392054, mean reconstruction loss=74.2686734009\n",
      "('epoch', 368)\n",
      "train mean loss=99.1379763412, mean reconstruction loss=74.2574167124\n",
      "('epoch', 369)\n",
      "train mean loss=99.1548350016, mean reconstruction loss=74.2552373759\n",
      "('epoch', 370)\n",
      "train mean loss=99.1326033656, mean reconstruction loss=74.2504946391\n",
      "('epoch', 371)\n",
      "train mean loss=99.1467570496, mean reconstruction loss=74.2671782811\n",
      "('epoch', 372)\n",
      "train mean loss=99.1191348521, mean reconstruction loss=74.2157907359\n",
      "('epoch', 373)\n",
      "train mean loss=99.1268001175, mean reconstruction loss=74.2291787084\n",
      "('epoch', 374)\n",
      "train mean loss=99.1237189102, mean reconstruction loss=74.2383443832\n",
      "('epoch', 375)\n",
      "train mean loss=99.1154114405, mean reconstruction loss=74.2339761225\n",
      "('epoch', 376)\n",
      "train mean loss=99.0964335887, mean reconstruction loss=74.2120259603\n",
      "('epoch', 377)\n",
      "train mean loss=99.0898562113, mean reconstruction loss=74.2362312953\n",
      "('epoch', 378)\n",
      "train mean loss=99.0795838038, mean reconstruction loss=74.215790151\n",
      "('epoch', 379)\n",
      "train mean loss=99.118743426, mean reconstruction loss=74.2469559224\n",
      "('epoch', 380)\n",
      "train mean loss=99.0826338323, mean reconstruction loss=74.217457339\n",
      "('epoch', 381)\n",
      "train mean loss=99.1529775238, mean reconstruction loss=74.2709675217\n",
      "('epoch', 382)\n",
      "train mean loss=99.0825854492, mean reconstruction loss=74.2143940099\n",
      "('epoch', 383)\n",
      "train mean loss=99.0919889832, mean reconstruction loss=74.2187117767\n",
      "('epoch', 384)\n",
      "train mean loss=99.0479612732, mean reconstruction loss=74.1856587601\n",
      "('epoch', 385)\n",
      "train mean loss=99.0939388148, mean reconstruction loss=74.2071466319\n",
      "('epoch', 386)\n",
      "train mean loss=99.0702022552, mean reconstruction loss=74.2166123327\n",
      "('epoch', 387)\n",
      "train mean loss=99.0801022593, mean reconstruction loss=74.2152702459\n",
      "('epoch', 388)\n",
      "train mean loss=99.0891477839, mean reconstruction loss=74.2243413798\n",
      "('epoch', 389)\n",
      "train mean loss=99.0555978012, mean reconstruction loss=74.2058334351\n",
      "('epoch', 390)\n",
      "train mean loss=99.0607103602, mean reconstruction loss=74.2349798075\n",
      "('epoch', 391)\n",
      "train mean loss=99.0529967753, mean reconstruction loss=74.2016468557\n",
      "('epoch', 392)\n",
      "train mean loss=99.0530313873, mean reconstruction loss=74.1977180608\n",
      "('epoch', 393)\n",
      "train mean loss=99.0531983948, mean reconstruction loss=74.2032991791\n",
      "('epoch', 394)\n",
      "train mean loss=99.0864137268, mean reconstruction loss=74.2297694524\n",
      "('epoch', 395)\n",
      "train mean loss=99.0220588175, mean reconstruction loss=74.1982989502\n",
      "('epoch', 396)\n",
      "train mean loss=98.984287707, mean reconstruction loss=74.1518243663\n",
      "('epoch', 397)\n",
      "train mean loss=99.0908479945, mean reconstruction loss=74.2219574483\n",
      "('epoch', 398)\n",
      "train mean loss=99.0514753469, mean reconstruction loss=74.1685848109\n",
      "('epoch', 399)\n",
      "train mean loss=98.9984066264, mean reconstruction loss=74.1518449783\n",
      "('epoch', 400)\n",
      "train mean loss=99.057479922, mean reconstruction loss=74.2014082718\n",
      "('epoch', 401)\n",
      "train mean loss=99.0123154068, mean reconstruction loss=74.1649942271\n",
      "('epoch', 402)\n",
      "train mean loss=99.0484157181, mean reconstruction loss=74.2032262293\n",
      "('epoch', 403)\n",
      "train mean loss=99.0313491313, mean reconstruction loss=74.1958144506\n",
      "('epoch', 404)\n",
      "train mean loss=99.0768490092, mean reconstruction loss=74.1768040848\n",
      "('epoch', 405)\n",
      "train mean loss=98.9927375285, mean reconstruction loss=74.1560646693\n",
      "('epoch', 406)\n",
      "train mean loss=99.0275190989, mean reconstruction loss=74.1833557002\n",
      "('epoch', 407)\n",
      "train mean loss=99.0283815893, mean reconstruction loss=74.1918333308\n",
      "('epoch', 408)\n",
      "train mean loss=99.0172372437, mean reconstruction loss=74.1744838842\n",
      "('epoch', 409)\n",
      "train mean loss=98.9696522903, mean reconstruction loss=74.1467490896\n",
      "('epoch', 410)\n",
      "train mean loss=99.0099779129, mean reconstruction loss=74.1757595062\n",
      "('epoch', 411)\n",
      "train mean loss=98.9779385249, mean reconstruction loss=74.1402424367\n",
      "('epoch', 412)\n",
      "train mean loss=98.9688175074, mean reconstruction loss=74.1342740122\n",
      "('epoch', 413)\n",
      "train mean loss=99.0302254613, mean reconstruction loss=74.1787727102\n",
      "('epoch', 414)\n",
      "train mean loss=98.9988664118, mean reconstruction loss=74.158220253\n",
      "('epoch', 415)\n",
      "train mean loss=98.9880691655, mean reconstruction loss=74.1790312958\n",
      "('epoch', 416)\n",
      "train mean loss=99.0185678609, mean reconstruction loss=74.1907196426\n",
      "('epoch', 417)\n",
      "train mean loss=98.9475108846, mean reconstruction loss=74.1278047943\n",
      "('epoch', 418)\n",
      "train mean loss=99.0182383728, mean reconstruction loss=74.1647469966\n",
      "('epoch', 419)\n",
      "train mean loss=98.9931899134, mean reconstruction loss=74.1637779744\n",
      "('epoch', 420)\n",
      "train mean loss=98.9664206441, mean reconstruction loss=74.133674113\n",
      "('epoch', 421)\n",
      "train mean loss=99.0017201106, mean reconstruction loss=74.1500573603\n",
      "('epoch', 422)\n",
      "train mean loss=99.0359291331, mean reconstruction loss=74.191133817\n",
      "('epoch', 423)\n",
      "train mean loss=98.9447791672, mean reconstruction loss=74.1288632584\n",
      "('epoch', 424)\n",
      "train mean loss=98.9205880864, mean reconstruction loss=74.0879225032\n",
      "('epoch', 425)\n",
      "train mean loss=98.979858106, mean reconstruction loss=74.1303565725\n",
      "('epoch', 426)\n",
      "train mean loss=98.9353146362, mean reconstruction loss=74.1311339696\n",
      "('epoch', 427)\n",
      "train mean loss=98.9259664536, mean reconstruction loss=74.1185299301\n",
      "('epoch', 428)\n",
      "train mean loss=98.9050021998, mean reconstruction loss=74.1119887797\n",
      "('epoch', 429)\n",
      "train mean loss=98.9212523524, mean reconstruction loss=74.1230051041\n",
      "('epoch', 430)\n",
      "train mean loss=98.9569164403, mean reconstruction loss=74.1359683736\n",
      "('epoch', 431)\n",
      "train mean loss=98.914605306, mean reconstruction loss=74.1142709732\n",
      "('epoch', 432)\n",
      "train mean loss=98.9262039185, mean reconstruction loss=74.1124157206\n",
      "('epoch', 433)\n",
      "train mean loss=98.9716540146, mean reconstruction loss=74.1218278122\n",
      "('epoch', 434)\n",
      "train mean loss=98.9366822942, mean reconstruction loss=74.1558813477\n",
      "('epoch', 435)\n",
      "train mean loss=98.9245781962, mean reconstruction loss=74.1118519338\n",
      "('epoch', 436)\n",
      "train mean loss=98.8951612218, mean reconstruction loss=74.0936752701\n",
      "('epoch', 437)\n",
      "train mean loss=98.9337283452, mean reconstruction loss=74.1198661931\n",
      "('epoch', 438)\n",
      "train mean loss=98.9277445857, mean reconstruction loss=74.1267049408\n",
      "('epoch', 439)\n",
      "train mean loss=98.9273947271, mean reconstruction loss=74.1379561996\n",
      "('epoch', 440)\n",
      "train mean loss=98.8756847, mean reconstruction loss=74.0797881063\n",
      "('epoch', 441)\n",
      "train mean loss=98.9150856908, mean reconstruction loss=74.1084482447\n",
      "('epoch', 442)\n",
      "train mean loss=98.8653765742, mean reconstruction loss=74.0959094747\n",
      "('epoch', 443)\n",
      "train mean loss=98.8862710444, mean reconstruction loss=74.0916377258\n",
      "('epoch', 444)\n",
      "train mean loss=98.9225738653, mean reconstruction loss=74.1211017609\n",
      "('epoch', 445)\n",
      "train mean loss=98.8980648422, mean reconstruction loss=74.1051450729\n",
      "('epoch', 446)\n",
      "train mean loss=98.8993769964, mean reconstruction loss=74.0886523565\n",
      "('epoch', 447)\n",
      "train mean loss=98.9070674133, mean reconstruction loss=74.1047811762\n",
      "('epoch', 448)\n",
      "train mean loss=98.8928167216, mean reconstruction loss=74.103899676\n",
      "('epoch', 449)\n",
      "train mean loss=98.8675629171, mean reconstruction loss=74.0717662684\n",
      "('epoch', 450)\n",
      "train mean loss=98.9108813222, mean reconstruction loss=74.1131047821\n",
      "('epoch', 451)\n",
      "train mean loss=98.8373800151, mean reconstruction loss=74.0518055344\n",
      "('epoch', 452)\n",
      "train mean loss=98.8756119537, mean reconstruction loss=74.0668224589\n",
      "('epoch', 453)\n",
      "train mean loss=98.8609891891, mean reconstruction loss=74.0889822133\n",
      "('epoch', 454)\n",
      "train mean loss=98.8439088186, mean reconstruction loss=74.0758467865\n",
      "('epoch', 455)\n",
      "train mean loss=98.8278998057, mean reconstruction loss=74.072547582\n",
      "('epoch', 456)\n",
      "train mean loss=98.8520439784, mean reconstruction loss=74.0827302806\n",
      "('epoch', 457)\n",
      "train mean loss=98.8607593918, mean reconstruction loss=74.0934671783\n",
      "('epoch', 458)\n",
      "train mean loss=98.8608722051, mean reconstruction loss=74.0772922389\n",
      "('epoch', 459)\n",
      "train mean loss=98.879663264, mean reconstruction loss=74.0850292714\n",
      "('epoch', 460)\n",
      "train mean loss=98.8451618958, mean reconstruction loss=74.0708248647\n",
      "('epoch', 461)\n",
      "train mean loss=98.8383080419, mean reconstruction loss=74.0746685282\n",
      "('epoch', 462)\n",
      "train mean loss=98.8523834356, mean reconstruction loss=74.0843748474\n",
      "('epoch', 463)\n",
      "train mean loss=98.81849219, mean reconstruction loss=74.0649938202\n",
      "('epoch', 464)\n",
      "train mean loss=98.8281059265, mean reconstruction loss=74.0722304153\n",
      "('epoch', 465)\n",
      "train mean loss=98.8700920614, mean reconstruction loss=74.0772571437\n",
      "('epoch', 466)\n",
      "train mean loss=98.8269266383, mean reconstruction loss=74.0517468135\n",
      "('epoch', 467)\n",
      "train mean loss=98.8477947744, mean reconstruction loss=74.0933856837\n",
      "('epoch', 468)\n",
      "train mean loss=98.8644606272, mean reconstruction loss=74.0883248647\n",
      "('epoch', 469)\n",
      "train mean loss=98.8165285365, mean reconstruction loss=74.0422842153\n",
      "('epoch', 470)\n",
      "train mean loss=98.822435023, mean reconstruction loss=74.0394814809\n",
      "('epoch', 471)\n",
      "train mean loss=98.8592707062, mean reconstruction loss=74.0736546834\n",
      "('epoch', 472)\n",
      "train mean loss=98.7622119522, mean reconstruction loss=74.0253389994\n",
      "('epoch', 473)\n",
      "train mean loss=98.8296415329, mean reconstruction loss=74.0547988764\n",
      "('epoch', 474)\n",
      "train mean loss=98.8705590566, mean reconstruction loss=74.1027621969\n",
      "('epoch', 475)\n",
      "train mean loss=98.8154123942, mean reconstruction loss=74.0673812358\n",
      "('epoch', 476)\n",
      "train mean loss=98.7687354024, mean reconstruction loss=74.0171071879\n",
      "('epoch', 477)\n",
      "train mean loss=98.788564415, mean reconstruction loss=74.0478473028\n",
      "('epoch', 478)\n",
      "train mean loss=98.8008894475, mean reconstruction loss=74.0371283722\n",
      "('epoch', 479)\n",
      "train mean loss=98.826928393, mean reconstruction loss=74.0712348557\n",
      "('epoch', 480)\n",
      "train mean loss=98.7705574671, mean reconstruction loss=74.0289968363\n",
      "('epoch', 481)\n",
      "train mean loss=98.7392269262, mean reconstruction loss=74.0068901952\n",
      "('epoch', 482)\n",
      "train mean loss=98.8508725103, mean reconstruction loss=74.0753723145\n",
      "('epoch', 483)\n",
      "train mean loss=98.8046009445, mean reconstruction loss=74.0624997965\n",
      "('epoch', 484)\n",
      "train mean loss=98.8466665141, mean reconstruction loss=74.0752781423\n",
      "('epoch', 485)\n",
      "train mean loss=98.8291494242, mean reconstruction loss=74.0585028076\n",
      "('epoch', 486)\n",
      "train mean loss=98.7131084569, mean reconstruction loss=74.0194007746\n",
      "('epoch', 487)\n",
      "train mean loss=98.7933066432, mean reconstruction loss=74.0355551656\n",
      "('epoch', 488)\n",
      "train mean loss=98.7733266195, mean reconstruction loss=74.0655582428\n",
      "('epoch', 489)\n",
      "train mean loss=98.7588821157, mean reconstruction loss=74.0245415878\n",
      "('epoch', 490)\n",
      "train mean loss=98.7892028046, mean reconstruction loss=74.0528194809\n",
      "('epoch', 491)\n",
      "train mean loss=98.7579864883, mean reconstruction loss=74.0155129496\n",
      "('epoch', 492)\n",
      "train mean loss=98.7966045507, mean reconstruction loss=74.0384280014\n",
      "('epoch', 493)\n",
      "train mean loss=98.7742921066, mean reconstruction loss=74.0466292063\n",
      "('epoch', 494)\n",
      "train mean loss=98.7537571081, mean reconstruction loss=74.0299566905\n",
      "('epoch', 495)\n",
      "train mean loss=98.7601595815, mean reconstruction loss=74.0287815348\n",
      "('epoch', 496)\n",
      "train mean loss=98.7173826218, mean reconstruction loss=74.0040452194\n",
      "('epoch', 497)\n",
      "train mean loss=98.7370265198, mean reconstruction loss=74.027930247\n",
      "('epoch', 498)\n",
      "train mean loss=98.7824144999, mean reconstruction loss=74.0513445663\n",
      "('epoch', 499)\n",
      "train mean loss=98.7447513707, mean reconstruction loss=74.0346911367\n",
      "('epoch', 500)\n",
      "train mean loss=98.7626037852, mean reconstruction loss=74.0259408569\n"
     ]
    }
   ],
   "source": [
    "for epoch in six.moves.range(1, n_epoch + 1):\n",
    "    print('epoch', epoch)\n",
    "    \n",
    "    # training\n",
    "    ## 訓練データのsampler\n",
    "    perm = np.random.permutation(N)\n",
    "    ## lossのbuffer\n",
    "    sum_loss = 0       # total loss\n",
    "    sum_rec_loss = 0   # reconstruction loss\n",
    "    ## バッチ学習\n",
    "    for i in six.moves.range(0, N, batchsize):\n",
    "        x = chainer.Variable(xp.asarray(x_train[perm[i:i + batchsize]])) # バッチ分のデータの抽出\n",
    "        # optimizerのupdateメソッドにloss関数とデータを与える　こうすると中でいろいろやってくれる\n",
    "        optimizer.update(model.get_loss_func(), x)\n",
    "        # 勾配を明示的に計算する場合\n",
    "        ## loss-functionがちょっとわかんないので、後で試す\n",
    "        #model.zerograds()\n",
    "        #loss = model(x, t)\n",
    "        #loss.backward()\n",
    "        #optimizer.update()\n",
    "        \n",
    "        # chainer.computational_graphは1.7からの実装\n",
    "\n",
    "        sum_loss += float(model.loss.data) * len(x.data)\n",
    "        sum_rec_loss += float(model.rec_loss.data) * len(x.data)\n",
    "\n",
    "    print('train mean loss={}, mean reconstruction loss={}'.format(sum_loss / N, sum_rec_loss / N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training image\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAB+CAYAAABbJAkYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIsD/AdA3OWZ6GWyuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d8855bad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction image\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAB+CAYAAABbJAkYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH5BUfWXuf4KAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d4d242e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ind = [1, 3, 5, 10, 2, 0, 13, 15, 17]\n",
    "x = chainer.Variable(xp.asarray(x_train[train_ind]), volatile='on')\n",
    "x1 = model(x)\n",
    "\n",
    "print \"training image\"\n",
    "draw_digit2(x_train[train_ind])\n",
    "\n",
    "print \"reconstruction image\"\n",
    "draw_digit2(x1.data.get())\n",
    "#draw_digit2(x1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decode image from random vector\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAB+CAYAAABbJAkYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d4c902610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw images from randomly sampled z\n",
    "z = chainer.Variable(xp.random.normal(0, 1, (9, n_latent)).astype(np.float32))\n",
    "x = model.decode(z)\n",
    "#draw_digit2(x.data)\n",
    "print \"decode image from random vector\"\n",
    "draw_digit2(x.data.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
