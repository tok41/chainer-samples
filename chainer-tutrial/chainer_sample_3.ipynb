{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainerチュートリアル\n",
    "*http://docs.chainer.org/en/stable/tutorial/recurrentnet.html\n",
    "* Recurrent Net (full backprop, truncated backprop) を学ぶ\n",
    "* 様々な長さの入力時系列を扱う\n",
    "* forward計算の間、ネットワークの上流を切り捨てる\n",
    "  * 計算量の削減をするということか？　RNNの過去分を計算しないとか?\n",
    "* ネットワーク構造を避けるために(?) 揮発性変数(Volatile Valiables)を使う\n",
    "  * ?? LSTMのこと？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recurrent Nets\n",
    "* RecurrentNet とはloopを持ったニューラルネットワークで、入力シーケンス$x_1, x_2, \\cdots, x_t$と初期状態$h_0$を与えられる。そして、$h_t = f\\left( x_t, h_{t-1} \\right)$ によって繰り返し計算する。\n",
    "* 出力は$y_t = \\right( h_t \\left)$\n",
    "* この手順を拡大すると、通常のフィードフォワードネットワークのように解釈できる\n",
    "* \n",
    "* シンプルなRNNの例として、１層のlanguage-modelを学ぶ\n",
    "* これは、有限な単語列を与えられ、続く単語を予測したい。\n",
    "* 1000の異なる単語を想定しており、それぞれの単語は１００次元のベクトルで表現される\n",
    "* \n",
    "* ニューラルネット言語モデルにLSTM(chainer.links.LSTM)が使える。\n",
    "* このlinkは、普通のfully-connected層のように見える。宣言時に、入力と出力のサイズを設定する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = L.LSTM(100, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* forward計算を行う前に、LSTM層のリセットをする必要がある\n",
    "* LSTM層には内部状態を持っている（過去の状態など）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l.reset_state()\n",
    "x = Variable(np.random.randn(10, 100).astype(np.float32))\n",
    "y = l(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 次のステップからの入力は直接LSTM層に入力できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x2 = Variable(np.random.randn(10, 100).astype(np.float32))\n",
    "y2 = l(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LSTMを使ってRNNを書いてみる\n",
    "*\n",
    "* EmbedIDは一致する固定次元のembedding vectorに入力整数を変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(Chain):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__(\n",
    "            embed=L.EmbedID(1000, 100),  # word embedding\n",
    "            mid=L.LSTM(100, 50),  # the first LSTM layer\n",
    "            out=L.Linear(50, 1000),  # the feed-forward output layer\n",
    "        )\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.mid.reset_state()\n",
    "\n",
    "    def __call__(self, cur_word):\n",
    "        # Given the current word ID, predict the next word.\n",
    "        x = self.embed(cur_word)\n",
    "        h = self.mid(x)\n",
    "        y = self.out(h)\n",
    "        return y\n",
    "\n",
    "rnn = RNN()\n",
    "model = L.Classifier(rnn)\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* word変数のリストx_listを使って、シンプルなforループで損失を計算できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(x_list):\n",
    "    loss = 0\n",
    "    for cur_word, next_word in zip(x_list, x_list[1:]):\n",
    "        loss += model(cur_word, next_word)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上記の積み重ねられた損失変数には、過去の計算の履歴が含まれている\n",
    "* backward()メソッドを使ってモデルパラメータの勾配を計算できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-513695305e45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzerograds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Suppose we have a list of word variables x_list.\n",
    "rnn.reset_state()\n",
    "model.zerograds()\n",
    "loss = compute_loss(x_list)\n",
    "loss.backward()\n",
    "optimizer.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* データx_listを入力することでシーケンスを扱える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncate the Graph by Unchaining\n",
    "* RNNの用途として、非常に長いシーケンスを扱いたい場合がある. \n",
    "* メモリに収まらないほど長いシーケンスの場合、短い時間幅にbackpropagationをきりつめる\n",
    "* これを\"truncated backprop\"と呼んでいる\n",
    "* この手法はヒューリスティックであり、勾配の偏りを生む。しかし、とても長いシーケンスを扱う場合によく機能する\n",
    "* Chainerでは、\"backward unchaining\"と呼ぶ仕組みで、簡単に\"truncated backprop\"を実装できる\n",
    "* Variableオブジェクトを使い計算を始める。自動的にシーケンスの切り分けが実施される\n",
    "* 結果として、長くない計算履歴を持つ\n",
    "* \n",
    "* 上記のRNNと同じネットワークで\"backward unchain\"を使って描く\n",
    "* とても長いシーケンスに対して、３０ステップ毎に\"truncated backprop\"させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "count = 0\n",
    "seqlen = len(x_list[1:])\n",
    "\n",
    "rnn.reset_state()\n",
    "for cur_word, next_word in zip(x_list, x_list[1:]):\n",
    "    loss += model(cur_word, next_word)\n",
    "    count += 1\n",
    "    if count % 30 == 0 or count == seqlen:\n",
    "        model.zerograds()\n",
    "        loss.backward()\n",
    "        loss.unchain_backward()\n",
    "        optimizer.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model()でStateがアップデートされ、lossに損失が蓄積される\n",
    "* 累積損失から後方へ計算履歴を削除する時にunchain_backwardメソッドが呼ばれる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
